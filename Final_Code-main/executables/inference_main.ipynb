{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append('../src')\n",
    "\n",
    "from inference_utils import P1_inferecening, P2_inferencing, P3_inferencing\n",
    "from evaluator import evaluator, clean_output_p1, clean_output_p2_p3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference for P1 Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference for Static Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_df = pd.read_csv(\"../datasets/Pre-Generated/P1_datasets/test/StaticP1dataset_test.csv\")\n",
    "model = \"../RtaC-Models/deepseek-ai/deepseek-coder-1.3b-instruct\"\n",
    "\n",
    "if not os.path.exists('../output/P1_infered_output'):\n",
    "    os.makedirs('../output/P1_infered_output')\n",
    "\n",
    "static_output = []\n",
    "P1_inferencing_for_Static = P1_inferecening()\n",
    "for i, row in static_df.iterrows():\n",
    "    static_dict = row['Query']\n",
    "    ans, latency = P1_inferencing_for_Static.get_inference(model,static_dict)\n",
    "    static_output.append({'Output': ans , 'Ground_Truth': row['Output'],'Latency': latency})\n",
    "\n",
    "field_names= ['Output','Ground_Truth','Latency']\n",
    "\n",
    "with open('../output/P1_infered_output/staticInference.csv', 'w', encoding='utf-8') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(static_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference for Dynamic Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_df = pd.read_csv(\"../datasets/Pre-Generated/P1_datasets/test/DynamicP1dataset_test.csv\")\n",
    "model_name = \"../RtaC-Models/deepseek-ai/deepseek-coder-1.3b-instruct\"\n",
    "\n",
    "if not os.path.exists('../output/P1_infered_output'):\n",
    "    os.makedirs('../output/P1_infered_output')\n",
    "\n",
    "dynamic_output = []\n",
    "P1_inferencing_for_dynamic =P1_inferecening()\n",
    "for i, row in dynamic_df.iterrows():\n",
    "    dynamic_dict = row['Query']\n",
    "    ans, latency = P1_inferencing_for_Static.get_inference(model,dynamic_dict)\n",
    "    dynamic_output.append({'Output': ans , 'Ground_Truth': row['Output'],'Latency': latency})\n",
    "\n",
    "field_names= ['Output','Ground_Truth','Latency']\n",
    "\n",
    "with open('../output/P1_infered_output/dynamicInference.csv', 'w',encoding='utf-8') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(dynamic_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference for Bonus Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bonus_df = pd.read_csv(\"../datasets/Pre-Generated/P1_datasets/test/BonusP1dataset_test.csv\")\n",
    "model_name = \"../RtaC-Models/deepseek-ai/deepseek-coder-1.3b-instruct\"\n",
    "\n",
    "if not os.path.exists('../output/P1_infered_output'):\n",
    "    os.makedirs('../output/P1_infered_output')\n",
    "\n",
    "bonus_output = []\n",
    "P1_inferencing_for_bonus = P1_inferecening()\n",
    "for i, row in bonus_df.iterrows():\n",
    "    bonus_dict = row['Query']\n",
    "    ans, latency = P1_inferencing_for_Static.get_inference(model,bonus_dict)\n",
    "    bonus_output.append({'Output': ans , 'Ground_Truth': row['Output'],'Latency': latency})\n",
    "\n",
    "field_names= ['Output','Ground_Truth','Latency']\n",
    "\n",
    "with open('../output/P1_infered_output/bonusInference.csv', 'w',encoding='utf-8') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(bonus_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference for P2 Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference for Static Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_df = pd.read_csv(\"../datasets/Pre-Generated/P2_datasets/test/StaticP2prompt_test.csv\")\n",
    "\n",
    "model_name = \"../RtaC-Models/codellama/CodeLlama-7b-Instruct-hf\"\n",
    "infer_model = \"Insight244/codellama-p2-data-no-random-no-bonus\"\n",
    "\n",
    "if not os.path.exists('../output/P2_infered_output'):\n",
    "    os.makedirs('../output/P2_infered_output')\n",
    "\n",
    "P2_inferencing_for_Static = P2_inferencing()\n",
    "P2_inferencing_for_Static.P2_load_model(model_name, infer_model)\n",
    "\n",
    "static_output = []\n",
    "\n",
    "for i, row in static_df.iterrows():\n",
    "    static_dict = {'query': row['Prompt'],\n",
    "                   'output': row['Output']}\n",
    "    \n",
    "    ans, latency = P2_inferencing_for_Static.P2_get_inference(static_dict)\n",
    "\n",
    "    static_output.append({'Output': ans , 'Ground_Truth': row['Output'],'Latency': latency})\n",
    "\n",
    "field_names= ['Output','Ground_Truth','Latency']\n",
    "\n",
    "with open('../output/P2_infered_output/staticInference.csv', 'w', encoding='utf-8') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(static_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference for Dynamic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_df = pd.read_csv(\"../datasets/Pre-Generated/P2_datasets/test/DynamicP2prompt_test.csv\")\n",
    "\n",
    "model_name = \"../RtaC-Models/codellama/CodeLlama-7b-Instruct-hf\"\n",
    "infer_model = \"Insight244/codellama-p2-data-no-random-no-bonus\"\n",
    "\n",
    "if not os.path.exists('../output/P2_infered_output'):\n",
    "    os.makedirs('../output/P2_infered_output')\n",
    "\n",
    "P2_inferencing_for_dynamic = P2_inferencing()\n",
    "P2_inferencing_for_dynamic.P2_load_model(model_name, infer_model)\n",
    "\n",
    "dynamic_output = []\n",
    "\n",
    "for i, row in dynamic_df.iterrows():\n",
    "    dynamic_dict = {'query': row['Prompt'],\n",
    "                   'output': row['Output'],}\n",
    "\n",
    "    ans, latency = P2_inferencing_for_dynamic.P2_get_inference(dynamic_dict)\n",
    "\n",
    "    dynamic_output.append({'Output': ans , 'Ground_Truth': row['Output'],'Latency': latency})\n",
    "\n",
    "field_names= ['Output','Ground_Truth','Latency']\n",
    "\n",
    "with open('../output/P2_infered_output/dynamicInference.csv', 'w', encoding='utf-8') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(dynamic_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference for Bonus Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bonus_df = pd.read_csv(\"../datasets/Pre-Generated/P2_datasets/test/BonusP2prompt_test.csv\")\n",
    "\n",
    "model_name = \"../RtaC-Models/codellama/CodeLlama-7b-Instruct-hf\"\n",
    "infer_model = \"Insight244/codellama-p2-data-no-random-no-bonus\"\n",
    "\n",
    "if not os.path.exists('../output/P2_infered_output'):\n",
    "    os.makedirs('../output/P2_infered_output')\n",
    "\n",
    "P2_inferencing_for_bonus = P2_inferencing()\n",
    "P2_inferencing_for_bonus.P2_load_model(model_name, infer_model)\n",
    "\n",
    "bonus_output = []\n",
    "\n",
    "for i, row in bonus_df.iterrows():\n",
    "    bonus_dict = {'query': row['Prompt'],\n",
    "                   'output': row['Output'],}\n",
    "\n",
    "    ans, latency = P2_inferencing_for_bonus.P2_get_inference(bonus_dict)\n",
    "\n",
    "    bonus_output.append({'Output': ans , 'Ground_Truth': row['Output'],'Latency': latency})\n",
    "\n",
    "field_names= ['Output','Ground_Truth','Latency']\n",
    "\n",
    "with open('../output/P2_infered_output/bonusInference.csv', 'w', encoding='utf-8') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(bonus_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference for P3 Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference for Static Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_df = pd.read_csv(\"../datasets/Pre-Generated/P3_datasets/test/StaticP3prompt_test.csv\")\n",
    "\n",
    "model_name = \"../RtaC-Models/codellama/CodeLlama-7b-Instruct-hf\"\n",
    "infer_model_stage_1 = \"../RtaC-Models/codellama/codellama-7b-final-stage-1\"\n",
    "infer_model_stage_2 = \"../RtaC-Models/codellama/codellama-7b-final-stage-2-if\"\n",
    "\n",
    "if not os.path.exists('../output/P3_infered_output'):\n",
    "    os.makedirs('../output/P3_infered_output')\n",
    "\n",
    "P3_inferencing_for_Static = P3_inferencing()\n",
    "P3_inferencing_for_Static.P3_load_model(model_name, infer_model_stage_1, infer_model_stage_2)\n",
    "\n",
    "static_output = []\n",
    "\n",
    "for i, row in static_df.iterrows():\n",
    "    static_dict = {'query': row['Prompt'],\n",
    "                   'output': row['Output'],}\n",
    "\n",
    "    ans, latency = P3_inferencing_for_Static.P3_get_inference(static_dict)\n",
    "\n",
    "    static_output.append({'Output': ans , 'Ground_Truth': row['Output'],'Latency': latency})\n",
    "\n",
    "field_names= ['Output','Ground_Truth','Latency']\n",
    "\n",
    "with open('../output/P3_infered_output/staticInference.csv', 'w', encoding='utf-8') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(static_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference for Dynamic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_df = pd.read_csv(\"../datasets/Pre-Generated/P3_datasets/test/DynamicP3prompt_test.csv\")\n",
    "\n",
    "model_name = \"../RtaC-Models/codellama/CodeLlama-7b-Instruct-hf\"\n",
    "infer_model_stage_1 = \"../RtaC-Models/codellama/codellama-7b-final-stage-1\"\n",
    "infer_model_stage_2 = \"../RtaC-Models/codellama/codellama-7b-final-stage-2-if\"\n",
    "if not os.path.exists('../output/P3_infered_output'):\n",
    "    os.makedirs('../output/P3_infered_output')\n",
    "\n",
    "P3_inferencing_for_dynamic = P3_inferencing()\n",
    "P3_inferencing_for_dynamic.P3_load_model(model_name, infer_model_stage_1, infer_model_stage_2)\n",
    "\n",
    "dynamic_output = []\n",
    "\n",
    "for i, row in dynamic_df.iterrows():\n",
    "    dynamic_dict = {'query': row['Prompt'],\n",
    "                   'output': row['Output'],}\n",
    "\n",
    "    ans, latency = P3_inferencing_for_dynamic.P3_get_inference(dynamic_dict)\n",
    "\n",
    "    dynamic_output.append({'Output': ans , 'Ground_Truth': row['Output'],'Latency': latency})\n",
    "\n",
    "field_names= ['Output','Ground_Truth','Latency']\n",
    "\n",
    "with open('../output/P3_infered_output/dynamicInference.csv', 'w', encoding='utf-8') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(dynamic_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference for Bonus Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bonus_df = pd.read_csv(\"../datasets/Pre-Generated/P3_datasets/test/BonusP3prompt_test.csv\")\n",
    "\n",
    "model_name = \"../RtaC-Models/codellama/CodeLlama-7b-Instruct-hf\"\n",
    "infer_model_stage_1 = \"../RtaC-Models/codellama/codellama-7b-final-stage-1\"\n",
    "infer_model_stage_2 = \"../RtaC-Models/codellama/codellama-7b-final-stage-2-if\"\n",
    "\n",
    "if not os.path.exists('../output/P3_infered_output'):\n",
    "    os.makedirs('../output/P3_infered_output')\n",
    "\n",
    "P3_inferencing_for_bonus = P3_inferencing()\n",
    "P3_inferencing_for_bonus.P3_load_model(model_name, infer_model_stage_1, infer_model_stage_2)\n",
    "\n",
    "bonus_output = []\n",
    "\n",
    "for i, row in bonus_df.iterrows():\n",
    "    bonus_dict = {'query': row['Prompt'],\n",
    "                   'output': row['Output']}\n",
    "\n",
    "    ans, latency = P3_inferencing_for_bonus.P3_get_inference(bonus_dict)\n",
    "\n",
    "    bonus_output.append({'Output': ans , 'Ground_Truth': row['Output'],'Latency': latency})\n",
    "\n",
    "field_names= ['Output','Ground_Truth','Latency']\n",
    "\n",
    "with open('../output/P3_infered_output/bonusInference.csv', 'w', encoding='utf-8') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(bonus_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of P1 Pipeline on Static Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.read_csv(\"../output/P1_infered_output/staticInference.csv\")\n",
    "eval_df['Output'] = eval_df['Output'].apply(lambda x: clean_output_p1(x))\n",
    "precision, recall, F1, langeval, latency = evaluator(eval_df)\n",
    "\n",
    "print(\"Evaluation of P1 pipeline on Static Dataset\")\n",
    "print(\"Precision = \", precision)\n",
    "print(\"Recall = \", recall)\n",
    "print(\"F1 = \", F1)\n",
    "print(\"LangChain Metric = \", langeval)\n",
    "print(\"Latency = \" , latency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of P1 Pipeline on Dynamic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.read_csv(\"../output/P1_infered_output/dynamicInference.csv\")\n",
    "eval_df['Output'] = eval_df['Output'].apply(lambda x: clean_output_p1(x))\n",
    "precision, recall, F1, langeval, latency = evaluator(eval_df)\n",
    "\n",
    "print(\"Evaluation of P1 pipeline on Dynamic Dataset\")\n",
    "print(\"Precision = \", precision)\n",
    "print(\"Recall = \", recall)\n",
    "print(\"F1 = \", F1)\n",
    "print(\"LangChain Metric = \", langeval)\n",
    "print(\"Latency = \" , latency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of P1 Pipeline on Bonus Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.read_csv(\"../output/P1_infered_output/bonusInference.csv\")\n",
    "eval_df['Output'] = eval_df['Output'].apply(lambda x: clean_output_p1(x))\n",
    "precision, recall, F1, langeval, latency = evaluator(eval_df.copy())\n",
    "\n",
    "print(\"Evaluation of P1 pipeline on Bonus Dataset\")\n",
    "print(\"Precision = \", precision)\n",
    "print(\"Recall = \", recall)\n",
    "print(\"F1 = \", F1)\n",
    "print(\"LangChain Metric = \", langeval)\n",
    "print(\"Latency = \" , latency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of P2 Pipeline on Static Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.read_csv(\"../output/P2_infered_output/staticInference.csv\")\n",
    "eval_df['Output'] = eval_df['Output'].apply(lambda x: clean_output_p2_p3(x))\n",
    "precision, recall, F1, langeval, latency = evaluator(eval_df)\n",
    "\n",
    "print(\"Evaluation of P2 pipeline on Static Dataset\")\n",
    "print(\"Precision = \", precision)\n",
    "print(\"Recall = \", recall)\n",
    "print(\"F1 = \", F1)\n",
    "print(\"LangChain Metric = \", langeval)\n",
    "print(\"Latency = \" , latency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of P2 Pipeline on Dynamic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.read_csv(\"../output/P2_infered_output/dynamicInference.csv\")\n",
    "eval_df['Output'] = eval_df['Output'].apply(lambda x: clean_output_p2_p3(x))\n",
    "\n",
    "precision, recall, F1, langeval, latency = evaluator(eval_df)\n",
    "\n",
    "print(\"Evaluation of P2 pipeline on Dynamic Dataset\")\n",
    "print(\"Precision = \", precision)\n",
    "print(\"Recall = \", recall)\n",
    "print(\"F1 = \", F1)\n",
    "print(\"LangChain Metric = \", langeval)\n",
    "print(\"Latency = \" , latency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of P2 Pipeline on Bonus Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.read_csv(\"../output/P2_infered_output/bonusInference.csv\")\n",
    "eval_df['Output'] = eval_df['Output'].apply(lambda x: clean_output_p2_p3(x))\n",
    "\n",
    "precision, recall, F1, langeval, latency = evaluator(eval_df)\n",
    "\n",
    "print(\"Evaluation of P2 pipeline on Bonus Dataset\")\n",
    "print(\"Precision = \", precision)\n",
    "print(\"Recall = \", recall)\n",
    "print(\"F1 = \", F1)\n",
    "print(\"LangChain Metric = \", langeval)\n",
    "print(\"Latency = \" , latency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of P3 Pipeline on Static Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.read_csv(\"../output/P3_infered_output/staticInference.csv\")\n",
    "eval_df['Output'] = eval_df['Output'].apply(lambda x: clean_output_p2_p3(x))\n",
    "\n",
    "precision, recall, F1, langeval, latency = evaluator(eval_df)\n",
    "\n",
    "print(\"Evaluation of P3 pipeline on Static Dataset\")\n",
    "print(\"Precision = \", precision)\n",
    "print(\"Recall = \", recall)\n",
    "print(\"F1 = \", F1)\n",
    "print(\"LangChain Metric = \", langeval)\n",
    "print(\"Latency = \" , latency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of P3 Pipeline on Dynamic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.read_csv(\"../output/P3_infered_output/dynamicInference.csv\")\n",
    "eval_df['Output'] = eval_df['Output'].apply(lambda x: clean_output_p2_p3(x))\n",
    "\n",
    "precision, recall, F1, langeval, latency = evaluator(eval_df)\n",
    "\n",
    "print(\"Evaluation of P3 pipeline on Dynamic Dataset\")\n",
    "print(\"Precision = \", precision)\n",
    "print(\"Recall = \", recall)\n",
    "print(\"F1 = \", F1)\n",
    "print(\"LangChain Metric = \", langeval)\n",
    "print(\"Latency = \" , latency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of P3 Pipeline on Bonus Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.read_csv(\"../output/P3_infered_output/bonusInference.csv\")\n",
    "eval_df['Output'] = eval_df['Output'].apply(lambda x: clean_output_p2_p3(x))\n",
    "\n",
    "precision, recall, F1, langeval, latency = evaluator(eval_df)\n",
    "\n",
    "print(\"Evaluation of P3 pipeline on Bonus Dataset\")\n",
    "print(\"Precision = \", precision)\n",
    "print(\"Recall = \", recall)\n",
    "print(\"F1 = \", F1)\n",
    "print(\"LangChain Metric = \", langeval)\n",
    "print(\"Latency = \" , latency)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (main, Dec 15 2022, 17:11:09) [Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
