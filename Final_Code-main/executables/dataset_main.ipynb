{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import csv\n",
    "import sys\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "sys.path.append('../src') \n",
    "\n",
    "from dataset_utils import Static_dataGen, Dynamic_dataGen, Bonus_dataGen, Preprocessing\n",
    "\n",
    "key = os.environ.get(\"OPEN_AI_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Static Dataset Creation\n",
    "Aim: To generate a set of query-output pairs using the original set of 9 tools\n",
    "\n",
    "Method: \n",
    "1. A set of 3-4 tools is sampled every iteration for query generation\n",
    "2. The sampled set of tools is passed to an LLM agent for query generation\n",
    "3. The query is then passed to another agent, along with their descriptions, to generate its completion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "staticDatagen = Static_dataGen(key)\n",
    "\n",
    "no_of_StaticQuery_CompletionPairs2beGen = 10\n",
    "\n",
    "data_dict = staticDatagen.genQuery(no_of_StaticQuery_CompletionPairs2beGen)\n",
    "\n",
    "if not os.path.exists('../datasets/Generated/raw_data'):\n",
    "    os.makedirs('../datasets/Generated/raw_data')\n",
    "\n",
    "field_names= ['Query','Output']\n",
    "\n",
    "with open('../datasets/Generated/raw_data/saveStaticdataset.csv', 'w') as csv_file:  \n",
    "    csv_writer = csv.DictWriter(csv_file, fieldnames=data_dict[0].keys())\n",
    "    csv_writer.writeheader()\n",
    "    csv_writer.writerows(data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Dataset Creation\n",
    "Aim: To generate a dynamic toolset, and combining them with the original toolset to obtain a set of query-output pairs\n",
    "\n",
    "Method (Dynamic Toolset Creation): \n",
    "1. 4 tools are sampled from the original toolset every iteration\n",
    "2. These tools are then passed to an agent, to generate similar tools\n",
    "\n",
    "Method (Query-Output Pair Generation): \n",
    "1. Random 10 tools along with the original 9 at a time are passed to the agent for generating queries. The model has the liberty to select any number of tools from this for query generation. \n",
    "2. Another agent then generates the completions for the query list\n",
    "(The query list is cleaned by code and manual intervention before passing to the second agent, and a similar process is followed for the final CSV creation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamicDatagen = Dynamic_dataGen(key)\n",
    "\n",
    "no_of_newTool2beAdded = 10\n",
    "\n",
    "no_of_DynamicQuery_CompletionPairs2beGen = 10\n",
    "\n",
    "if not os.path.exists('../datasets/Generated/raw_data'):\n",
    "    os.makedirs('../datasets/Generated/raw_data')\n",
    "\n",
    "dynamicDatagen.genDynamicTools(no_of_newTool2beAdded)\n",
    "\n",
    "data_dict = dynamicDatagen.genDynamicQueryOutputPair(no_of_DynamicQuery_CompletionPairs2beGen)\n",
    "\n",
    "field_names= ['Added_Tools','Query','Output']\n",
    "\n",
    "with open('../datasets/Generated/raw_data/saveDynamicData.csv', 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Dataset Creation\n",
    "Aim: To generate a set of query-output pairs which involves usage of conditional and iterative operators\n",
    "\n",
    "Method: \n",
    "1. Manual creation of a list of 5 such query-output pairs is done. \n",
    "2. These examples with a list of few relevant dynamic tools is combined with the original toolset in the query-generating agent. \n",
    "3. Finally we pass this list of queries in the completion agent. (At every step of output from the model, the data is cleaned before saving and passing to the further agents.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bonusDatagen = Bonus_dataGen(key)\n",
    "\n",
    "no_of_BonusQuery_CompletionPairs2beGen = 10\n",
    "\n",
    "if not os.path.exists('../datasets/Generated/raw_data'):\n",
    "    os.makedirs('../datasets/Generated/raw_data')\n",
    "\n",
    "data_dict = bonusDatagen.genBonusQueryOutputPair(no_of_BonusQuery_CompletionPairs2beGen)\n",
    "\n",
    "field_names= ['Query','Output']\n",
    "\n",
    "with open('../datasets/Generated/raw_data/saveBonusData.csv', 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restructuring Dataset For Different Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Formation for P1 Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_df = pd.read_csv(\"../datasets/Pre-Generated/raw_data/static_dataset.csv\") \n",
    "dynamic_df = pd.read_csv(\"../datasets/Pre-Generated/raw_data/dynamic_dataset.csv\") \n",
    "bonus_df = pd.read_csv(\"../datasets/Pre-Generated/raw_data/bonus_dataset.csv\", encoding= 'unicode_escape') \n",
    "bonusTool_list = [row[0] for row in csv.reader(open('../resources/Tool_list/final-bonus-toolset.csv', 'r'))]\n",
    "\n",
    "datasetForm = Preprocessing()\n",
    "\n",
    "if not os.path.exists('../datasets/Generated/P1_datasets/test'):\n",
    "    os.makedirs('../datasets/Generated/P1_datasets/test')\n",
    "\n",
    "#Static\n",
    "staticDictP1 = []\n",
    "\n",
    "for i, row in static_df.iterrows():\n",
    "    query = row['Query']\n",
    "    output = row['Output']\n",
    "    added_tools = datasetForm.p1_static()\n",
    "    prompt = datasetForm.prompt_p1_static_dynamic(query, added_tools)\n",
    "    staticDictP1.append({'Query': prompt,'Output' : output})\n",
    "\n",
    "field_names= ['Query', 'Output']\n",
    "\n",
    "P1_static_test = staticDictP1[0:round(0.1*len(staticDictP1))]\n",
    "\n",
    "with open('../datasets/Generated/P1_datasets/test/StaticP1dataset_test.csv', 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(P1_static_test)\n",
    "\n",
    "#Dynamic\n",
    "dynamicDictP1 = []\n",
    "\n",
    "for i, row in dynamic_df.iterrows():\n",
    "    query = row['Query']\n",
    "    output = row['Output']\n",
    "    additional_tools = ast.literal_eval(row['Added_Tools'].replace(\"['\", \"['''\").replace(\"']\", \"''']\"))\n",
    "    added_tools = datasetForm.p1_dynamic(additional_tools)\n",
    "    prompt = datasetForm.prompt_p1_static_dynamic(query, added_tools)\n",
    "    dynamicDictP1.append({'Query': prompt,'Output' : output})\n",
    "\n",
    "field_names= ['Query', 'Output']\n",
    "\n",
    "P1_dynamic_test = dynamicDictP1[0:round(0.33*len(dynamicDictP1))]\n",
    "\n",
    "\n",
    "with open('../datasets/Generated/P1_datasets/test/DynamicP1dataset_test.csv', 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(P1_dynamic_test)\n",
    "\n",
    "# Bonus\n",
    "bonusDictP1 = []\n",
    "for i, row in bonus_df.iterrows():\n",
    "    query = row['Query']\n",
    "    output = row['Output']\n",
    "    added_tools = datasetForm.p1_bonus(bonusTool_list)\n",
    "    prompt = datasetForm.prompt_p1_bonus(query, added_tools)\n",
    "    bonusDictP1.append({'Query': prompt,'Output' : output})\n",
    "\n",
    "field_names= ['Query', 'Output']\n",
    "\n",
    "P1_bonus_test = bonusDictP1[0:round(0.1*len(bonusDictP1))]\n",
    "\n",
    "with open('../datasets/Generated/P1_datasets/test/BonusP1dataset_test.csv', 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(P1_bonus_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt formation for P2 Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_df = pd.read_csv(\"../datasets/Pre-Generated/raw_data/static_dataset.csv\") \n",
    "dynamic_df = pd.read_csv(\"../datasets/Pre-Generated/raw_data/dynamic_dataset.csv\") \n",
    "bonus_df = pd.read_csv(\"../datasets/Pre-Generated/raw_data/bonus_dataset.csv\", encoding= 'unicode_escape') \n",
    "bonusTool_list = [row[0] for row in csv.reader(open('../resources/Tool_list/final-bonus-toolset.csv', 'r'))]\n",
    "\n",
    "datasetForm = Preprocessing()\n",
    "\n",
    "if not os.path.exists('../datasets/Generated/P2_datasets/train_val'):\n",
    "    os.makedirs('../datasets/Generated/P2_datasets/train_val')\n",
    "\n",
    "if not os.path.exists('../datasets/Generated/P2_datasets/test'):\n",
    "    os.makedirs('../datasets/Generated/P2_datasets/test')\n",
    "\n",
    "#Static\n",
    "staticDictP2 = []\n",
    "for i, row in static_df.iterrows():\n",
    "    query = row['Query']\n",
    "    output = row['Output']\n",
    "    prompt = datasetForm.prompt_p2_pipeline(query,output)\n",
    "    staticDictP2.append({'Prompt':prompt, 'Output' : output})\n",
    "\n",
    "field_names= ['Prompt', 'Output']\n",
    "\n",
    "P2_test_static = staticDictP2[0:round(0.1*len(staticDictP2))]\n",
    "\n",
    "with open('../datasets/Generated/P2_datasets/test/StaticP2prompt_test.csv', 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(P2_test_static)\n",
    "\n",
    "#Dynamic\n",
    "dynamicDictP2 = []\n",
    "for i, row in dynamic_df.iterrows():\n",
    "    query = row['Query']\n",
    "    output = row['Output']\n",
    "    additional_tools = ast.literal_eval(row['Added_Tools'].replace(\"['\", \"['''\").replace(\"']\", \"''']\"))\n",
    "    prompt = datasetForm.prompt_p2_pipeline(query,output,additional_tools)\n",
    "    dynamicDictP2.append({'Prompt':prompt, 'Output' : output})\n",
    "\n",
    "field_names= ['Prompt', 'Output']\n",
    "\n",
    "P2_test_dynamic = dynamicDictP2[0:round(0.33*len(dynamicDictP2))]\n",
    "\n",
    "with open('../datasets/Generated/P2_datasets/test/DynamicP2prompt_test.csv', 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(P2_test_dynamic)\n",
    "\n",
    "#Bonus\n",
    "bonusDictP2 = []\n",
    "for i, row in bonus_df.iterrows():\n",
    "    query = row['Query']\n",
    "    output = row['Output']\n",
    "    prompt = datasetForm.prompt_p2_pipeline(query,output,bonusTool_list)\n",
    "    bonusDictP2.append({'Prompt':prompt, 'Output' : output})\n",
    "\n",
    "field_names= ['Prompt', 'Output']\n",
    "\n",
    "P2_test_bonus = bonusDictP2[0:round(0.1*len(bonusDictP2))]\n",
    "\n",
    "with open('../datasets/Generated/P2_datasets/test/BonusP2prompt_test.csv', 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(P2_test_bonus)\n",
    "\n",
    "P2_train_val = staticDictP2[round(0.1*len(staticDictP2)):] + dynamicDictP2[round(0.33*len(dynamicDictP2)):] + bonusDictP2[round(0.9*len(bonusDictP2)):]\n",
    "random.shuffle(P2_train_val)\n",
    "\n",
    "P2_val = P2_train_val[0:round(0.1*len(P2_train_val))]\n",
    "P2_train = P2_train_val[round(0.1*len(P2_train_val)):]\n",
    "\n",
    "\n",
    "with open('../datasets/Generated/P2_datasets/train_val/P2prompt_train.csv', 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(P2_train)\n",
    "\n",
    "with open('../datasets/Generated/P2_datasets/train_val/P2prompt_val.csv', 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(P2_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt formation for P3 Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_df = pd.read_csv(\"../datasets/Pre-Generated/raw_data/static_dataset.csv\") \n",
    "dynamic_df = pd.read_csv(\"../datasets/Pre-Generated/raw_data/dynamic_dataset.csv\") \n",
    "bonus_df = pd.read_csv(\"../datasets/Pre-Generated/raw_data/bonus_dataset.csv\", encoding= 'unicode_escape') \n",
    "bonusTool_list = [row[0] for row in csv.reader(open('../resources/Tool_list/final-bonus-toolset.csv', 'r'))]\n",
    "\n",
    "datasetForm = Preprocessing()\n",
    "\n",
    "if not os.path.exists('../datasets/Generated/P3_datasets/train_val'):\n",
    "    os.makedirs('../datasets/Generated/P3_datasets/train_val')\n",
    "\n",
    "if not os.path.exists('../datasets/Generated/P3_datasets/test'):\n",
    "    os.makedirs('../datasets/Generated/P3_datasets/test')\n",
    "\n",
    "#Static\n",
    "staticDictP3 = []\n",
    "for i, row in static_df.iterrows():\n",
    "    query = row['Query']\n",
    "    output = row['Output']\n",
    "    prompt = datasetForm.prompt_p3_pipeline(query,output)\n",
    "    staticDictP3.append({'Prompt':prompt, 'Output' : output})\n",
    "\n",
    "field_names= ['Prompt', 'Output']\n",
    "\n",
    "P3_test_static = staticDictP3[0:round(0.1*len(staticDictP3))]\n",
    "\n",
    "with open('../datasets/Generated/P3_datasets/test/StaticP3prompt_test.csv', 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(P3_test_static)\n",
    "\n",
    "#Dynamic\n",
    "dynamicDictP3 = []\n",
    "for i, row in dynamic_df.iterrows():\n",
    "    query = row['Query']\n",
    "    output = row['Output']\n",
    "    additional_tools = ast.literal_eval(row['Added_Tools'].replace(\"['\", \"['''\").replace(\"']\", \"''']\"))\n",
    "    prompt = datasetForm.prompt_p3_pipeline(query,output,additional_tools)\n",
    "    dynamicDictP3.append({'Prompt':prompt, 'Output' : output})\n",
    "\n",
    "field_names= ['Prompt', 'Output']\n",
    "\n",
    "P3_test_dynamic = dynamicDictP3[0:round(0.33*len(dynamicDictP3))]\n",
    "\n",
    "with open('../datasets/Generated/P3_datasets/test/DynamicP3prompt_test.csv', 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(P3_test_dynamic)\n",
    "\n",
    "#Bonus\n",
    "bonusDictP3 = []\n",
    "for i, row in bonus_df.iterrows():\n",
    "    query = row['Query']\n",
    "    output = row['Output']\n",
    "    prompt = datasetForm.prompt_p3_pipeline(query,output,bonusTool_list)\n",
    "    bonusDictP3.append({'Prompt':prompt, 'Output' : output})\n",
    "\n",
    "field_names= ['Prompt', 'Output']\n",
    "\n",
    "P3_test_bonus = bonusDictP3[0:round(0.1*len(bonusDictP3))]\n",
    "\n",
    "with open('../datasets/Generated/P3_datasets/test/BonusP3prompt_test.csv', 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(P3_test_bonus)\n",
    "\n",
    "P3_train_val_stage_1 = staticDictP3[round(0.1*len(staticDictP3)):]\n",
    "P3_train_val_stage_2 = staticDictP3[round(0.75*len(staticDictP3)):] + bonusDictP3\n",
    "\n",
    "random.shuffle(P3_train_val_stage_1)\n",
    "random.shuffle(P3_train_val_stage_2)\n",
    "\n",
    "\n",
    "P3_val_stage_1 = P3_train_val_stage_1[0:round(0.1*len(P3_train_val_stage_1))]\n",
    "P3_train_stage_1 = P3_train_val_stage_1[round(0.1*len(P3_train_val_stage_1)):]\n",
    "\n",
    "P3_val_stage_2 = P3_train_val_stage_2[0:round(0.1*len(P3_train_val_stage_2))]\n",
    "P3_train_stage_2 = P3_train_val_stage_2[round(0.1*len(P3_train_val_stage_2)):]\n",
    "\n",
    "if not os.path.exists('../datasets/Generated/P3_datasets/train_val/Stage-1'):\n",
    "    os.makedirs('../datasets/Generated/P3_datasets/train_val/Stage-1')\n",
    "\n",
    "if not os.path.exists('../datasets/Generated/P3_datasets/train_val/Stage-2'):\n",
    "    os.makedirs('../datasets/Generated/P3_datasets/train_val/Stage-2')\n",
    "\n",
    "with open('../datasets/Generated/P3_datasets/train_val/Stage-1/P3prompt_stage_1_train.csv', 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(P3_train_stage_1)\n",
    "\n",
    "with open('../datasets/Generated/P3_datasets/train_val/Stage-1/P3prompt_stage_1_val.csv', 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(P3_val_stage_1)\n",
    "\n",
    "with open('../datasets/Generated/P3_datasets/train_val/Stage-2/P3prompt_stage_2_train.csv', 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(P3_train_stage_2)\n",
    "\n",
    "with open('../datasets/Generated/P3_datasets/train_val/Stage-2/P3prompt_stage_2_val.csv', 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(P3_val_stage_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev-rev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
